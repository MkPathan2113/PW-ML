{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7517c949",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    "application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5467e0d",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data normalization technique that rescales the features of a dataset to a specific range, typically between 0 and 1. It is used in data preprocessing to ensure that all features have a consistent scale, which can improve the performance of machine learning algorithms that are sensitive to the scale of features.\n",
    "\n",
    "Example:\n",
    "Let's say we have a dataset with a feature \"Income\" that ranges from $20,000 to $100,000. After applying Min-Max scaling, the values of this feature would be transformed to a range between 0 and 1, preserving the relative differences in values while ensuring they are on a consistent scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c53c4",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "Provide an example to illustrate its application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a486dc98",
   "metadata": {},
   "source": [
    "The Unit Vector technique in feature scaling involves scaling each feature so that it has a unit norm (magnitude or length) of 1. This technique normalizes the vectors by dividing each feature vector by its magnitude. Unlike Min-Max scaling, which rescales features to a specific range, Unit Vector scaling focuses on the direction of the vectors rather than their absolute values.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with two features, \"Height\" and \"Weight.\" After applying Unit Vector scaling, each data point's feature vector is divided by its magnitude (Euclidean norm), ensuring that all feature vectors have a length of 1. This normalization technique is particularly useful in scenarios where the direction of the feature vectors is more important than their magnitude, such as in text classification using word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03939981",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "example to illustrate its application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9204d2c6",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while preserving most of the variance in the data. It achieves this by finding the principal components, which are orthogonal vectors that represent the directions of maximum variance in the original data.\n",
    "\n",
    "Example:\n",
    "Let's say we have a dataset with multiple features such as age, income, education level, and spending habits. By applying PCA to this dataset, PCA will identify the principal components (linear combinations of the original features) that capture the most variance in the data. These principal components can be ranked by their importance, and we can choose to keep only the top components that explain a significant portion of the variance (e.g., 95%). The reduced dataset with fewer dimensions (features) can then be used for further analysis or modeling, reducing computational complexity and potential overfitting while retaining essential information from the original data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43855119",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d61f1c",
   "metadata": {},
   "source": [
    "PCA is a method often used for feature extraction, where high-dimensional data is transformed into a lower-dimensional representation while preserving essential information. By identifying principal components, PCA effectively creates new features that capture the most significant variance in the original data.\n",
    "\n",
    "Example:\n",
    "Consider a dataset with numerous customer behavior features. Applying PCA, we derive principal components, which are combinations of the original features that best capture variance. These components serve as new, condensed features, aiding tasks like classification or visualization, while reducing computational complexity and noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0ab91",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "preprocess the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b38db0b",
   "metadata": {},
   "source": [
    "### To preprocess the data for building a recommendation system for a food delivery service using Min-Max scaling, follow these steps:\n",
    "\n",
    "#### Understand the Data: \n",
    "Take a look at the dataset to understand the range and distribution of features such as price, rating, and delivery time.\n",
    "\n",
    "#### Min-Max Scaling:\n",
    "Min-Max scaling involves scaling features to a specific range, commonly between 0 and 1. Here's how you would apply it to each feature:\n",
    "\n",
    "Price: Let's say prices range from $5 to $50. You would use the Min-Max scaling formula: Xscaled = (Xi - Xmin)/(Xmax-Xmin)\n",
    "\n",
    "Rating: Ratings may be on a scale such as 1 to 5. Apply the same Min-Max scaling formula to scale the ratings between 0 and 1.\n",
    "\n",
    "Delivery Time: If delivery times range from 10 minutes to 60 minutes, again, use Min-Max scaling to scale these values.\n",
    "\n",
    "#### Apply to Dataset:\n",
    "Implement the Min-Max scaling transformation to all relevant features in your dataset, ensuring they are scaled appropriately within the desired range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba8106",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "dimensionality of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca3354",
   "metadata": {},
   "source": [
    "Prepare Data: Collect company financial and market trend data.<br>\n",
    "\n",
    "Standardize Data: Ensure all features have a similar scale.<br>\n",
    "\n",
    "Apply PCA: Use PCA to find important patterns in the data.<br>\n",
    "\n",
    "Select Components: Keep principal components explaining most variance.<br>\n",
    "\n",
    "Transform Data: Project data onto reduced-dimensional space.<br>\n",
    "\n",
    "Train Model: Use transformed data for stock price prediction model.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10e997c",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "values to a range of -1 to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a9c092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset: [ 1  5 10 15 20]\n",
      "Scaled Dataset [-1 to 1]: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the Min and Max values\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# Define the range for scaling\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = ((data - min_val) * (new_max - new_min) / (max_val - min_val)) + new_min\n",
    "\n",
    "# Print the scaled dataset\n",
    "print(\"Original Dataset:\", data)\n",
    "print(\"Scaled Dataset [-1 to 1]:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8caf83",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform \n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efe057",
   "metadata": {},
   "source": [
    "Standardize Data:\n",
    "Standardize numerical features (height, weight, age, blood pressure) to have zero mean and unit variance.\n",
    "\n",
    "Apply PCA:\n",
    "Use PCA on the standardized features to find principal components.\n",
    "\n",
    "Explained Variance:\n",
    "Check the explained variance ratio for each component.\n",
    "\n",
    "Cumulative Variance:\n",
    "Calculate the cumulative explained variance.\n",
    "\n",
    "Select Components:\n",
    "Choose the number of principal components to retain based on the cumulative explained variance. Aim for a threshold like 90% or 95% to retain significant variance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b5e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the dataset (features)\n",
    "# features = np.array([[height1, weight1, age1, gender1, bp1],\n",
    "#                      [height2, weight2, age2, gender2, bp2],\n",
    "#                      ...,\n",
    "#                      [heightn, weightn, agen, gendern, bpn]])\n",
    "\n",
    "# # Standardize the features (excluding gender)\n",
    "# scaler = StandardScaler()\n",
    "# features_scaled = scaler.fit_transform(features[:, :-1])  # Exclude gender\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA()\n",
    "# pca.fit(features_scaled)\n",
    "\n",
    "# # Calculate explained variance ratio and cumulative explained variance\n",
    "# explained_variance_ratio = pca.explained_variance_ratio_\n",
    "# cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# # Determine number of components to retain (e.g., 95% of variance)\n",
    "# n_components = np.argmax(cumulative_variance >= 0.95) + 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

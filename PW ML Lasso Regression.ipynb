{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f92b989",
   "metadata": {},
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0cc0e",
   "metadata": {},
   "source": [
    "1. **Definition**:\n",
    "   - **Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a linear regression method that includes an L1 penalty term to shrink coefficients.\n",
    "\n",
    "2. **Penalty Term**:\n",
    "   - Adds L1 regularization: \\(\\text{Cost Function} = \\text{OLS Loss} + \\lambda \\sum_{j} |\\beta_j|\\).\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   - Can set some coefficients to zero, performing automatic feature selection.\n",
    "\n",
    "4. **Comparison**:\n",
    "   - **Ridge Regression**: Uses L2 regularization (\\(\\lambda \\sum_{j} \\beta_j^2\\)), which shrinks but doesnâ€™t zero out coefficients.\n",
    "   - **Elastic Net**: Combines L1 and L2 regularization for a balance of feature selection and coefficient shrinkage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd912094",
   "metadata": {},
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44eef1b",
   "metadata": {},
   "source": [
    "\n",
    "1. **Automatic Feature Selection**:\n",
    "   - Lasso Regression can shrink some coefficients to exactly zero, effectively excluding less important features from the model.\n",
    "\n",
    "2. **Simplifies the Model**:\n",
    "   - By reducing the number of predictors, it helps in creating a simpler and more interpretable model.\n",
    "\n",
    "3. **Improves Model Performance**:\n",
    "   - Eliminating irrelevant features can reduce overfitting and improve the model's generalization to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2194c75b",
   "metadata": {},
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe928ffc",
   "metadata": {},
   "source": [
    "\n",
    "1. **Magnitude of Non-Zero Coefficients**:\n",
    "   - The magnitude of non-zero coefficients indicates the relative importance of each predictor variable.\n",
    "\n",
    "2. **Zero Coefficients**:\n",
    "   - Coefficients set to zero imply that the corresponding predictors have been excluded from the model.\n",
    "\n",
    "3. **Regularization Effect**:\n",
    "   - Lasso's L1 penalty shrinks coefficients, making the model simpler. Larger absolute values of coefficients suggest greater influence on the response variable.\n",
    "\n",
    "4. **Comparison to OLS**:\n",
    "   - Coefficients in Lasso are usually smaller and sparser than those from ordinary least squares (OLS) due to regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441db70",
   "metadata": {},
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37942ef",
   "metadata": {},
   "source": [
    "### What Are the Tuning Parameters That Can Be Adjusted in Lasso Regression, and How Do They Affect the Model's Performance?\n",
    "\n",
    "1. **Regularization Parameter (\\(\\lambda\\))**:\n",
    "   - **Definition**: Controls the strength of the L1 penalty.\n",
    "   - **Effect**: \n",
    "     - **High \\(\\lambda\\)**: Increases the penalty, leading to more coefficients being shrunk to zero. This results in a simpler model but may underfit the data.\n",
    "     - **Low \\(\\lambda\\)**: Reduces the penalty, keeping more coefficients non-zero. This can lead to a more complex model that may overfit.\n",
    "\n",
    "2. **Alpha Parameter (if using Elastic Net)**:\n",
    "   - **Definition**: Balances between L1 (Lasso) and L2 (Ridge) regularization.\n",
    "   - **Effect**: \n",
    "     - **High Alpha**: More emphasis on L1 regularization (like Lasso).\n",
    "     - **Low Alpha**: More emphasis on L2 regularization (like Ridge).\n",
    "\n",
    "Adjusting these parameters helps in balancing model complexity and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02a7ed4",
   "metadata": {},
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2874a8",
   "metadata": {},
   "source": [
    "\n",
    "1. **Direct Application**:\n",
    "   - Lasso Regression itself is designed for linear models and cannot handle non-linear relationships directly.\n",
    "\n",
    "2. **Feature Engineering**:\n",
    "   - Transform the data using non-linear features (e.g., polynomial features, interaction terms) to capture non-linear patterns.\n",
    "   - Apply Lasso Regression on the transformed features to achieve regularization and feature selection.\n",
    "\n",
    "3. **Kernel Methods**:\n",
    "   - Use kernel tricks to map data into higher-dimensional space where linear relationships may become apparent, then apply Lasso Regression.\n",
    "\n",
    "4. **Combine with Non-Linear Models**:\n",
    "   - Combine Lasso with non-linear models like decision trees or neural networks to leverage Lasso's feature selection capabilities.\n",
    "\n",
    "By transforming the data or combining with non-linear methods, Lasso Regression can be adapted for non-linear regression problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae1f8fa",
   "metadata": {},
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d805688",
   "metadata": {},
   "source": [
    "\n",
    "1. **Regularization Term**:\n",
    "   - **Ridge Regression**: Uses L2 regularization \\(\\lambda \\sum_{j} \\beta_j^2\\), which shrinks coefficients but does not set them to zero.\n",
    "   - **Lasso Regression**: Uses L1 regularization \\(\\lambda \\sum_{j} |\\beta_j|\\), which can shrink some coefficients to exactly zero.\n",
    "\n",
    "2. **Feature Selection**:\n",
    "   - **Ridge Regression**: Does not perform feature selection; all predictors remain in the model.\n",
    "   - **Lasso Regression**: Performs automatic feature selection by setting some coefficients to zero.\n",
    "\n",
    "3. **Model Complexity**:\n",
    "   - **Ridge Regression**: Tends to keep all features but reduces their impact, useful for handling multicollinearity.\n",
    "   - **Lasso Regression**: Creates a sparser model by excluding less important features.\n",
    "\n",
    "4. **Coefficient Shrinkage**:\n",
    "   - **Ridge Regression**: Shrinks coefficients evenly, leading to small but non-zero values.\n",
    "   - **Lasso Regression**: Can set coefficients to exactly zero, leading to a simpler and more interpretable model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0b003",
   "metadata": {},
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efac9f51",
   "metadata": {},
   "source": [
    "\n",
    "1. **Automatic Feature Selection**:\n",
    "   - **Lasso Regression** can handle multicollinearity by setting some coefficients to zero, effectively excluding highly correlated features from the model.\n",
    "\n",
    "2. **Reduces Complexity**:\n",
    "   - By removing redundant predictors, Lasso helps to simplify the model and reduce the impact of multicollinearity.\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - Lasso selects a subset of important features, which can alleviate issues caused by multicollinearity and improve model interpretability.\n",
    "\n",
    "4. **Limitations**:\n",
    "   - While Lasso addresses multicollinearity to some extent, it may still be beneficial to combine it with other techniques or regularization methods for better results.\n",
    "\n",
    "Lasso Regression helps manage multicollinearity by performing feature selection and reducing model complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f6d063",
   "metadata": {},
   "source": [
    "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f8d9e",
   "metadata": {},
   "source": [
    "\n",
    "1. **Cross-Validation**:\n",
    "   - Use k-fold cross-validation to evaluate model performance for different \\(\\lambda\\) values.\n",
    "   - Select the \\(\\lambda\\) that minimizes the cross-validation error.\n",
    "\n",
    "2. **Grid Search**:\n",
    "   - Perform a grid search over a range of \\(\\lambda\\) values.\n",
    "   - Choose the \\(\\lambda\\) that provides the best model performance.\n",
    "\n",
    "3. **Regularization Path**:\n",
    "   - Plot the regularization path to visualize how coefficients change with different \\(\\lambda\\) values.\n",
    "   - Select \\(\\lambda\\) where the coefficients stabilize.\n",
    "\n",
    "4. **Automated Tools**:\n",
    "   - Utilize built-in functions (e.g., `LassoCV` in scikit-learn) that automatically find the optimal \\(\\lambda\\) through cross-validation.\n",
    "\n",
    "By using these methods, you can effectively determine the optimal \\(\\lambda\\) for Lasso Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a51c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e5444d",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2e334c",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "\n",
    "**Definition:** Simple linear regression is a statistical method that models the relationship between a single independent variable (predictor) and a dependent variable (response) by fitting a linear equation to the observed data.\n",
    "\n",
    "**Example:** Predicting a person's weight based on their height.\n",
    "\n",
    "* Model y = b + b1x\n",
    "* Variables:\n",
    "y: Weight (dependent variable)\n",
    "x: Height (independent variable)\n",
    "\n",
    "\n",
    "## Multiple Linear Regression\n",
    "\n",
    "**Definition:** Multiple linear regression is an extension of simple linear regression that models the relationship between two or more independent variables and a dependent variable by fitting a linear equation to the observed data.\n",
    "\n",
    "**Example:** Predicting a person's weight based on their height and age.\n",
    "\n",
    "### Model\n",
    "\\[ y = b_0 + b_1 x_1 + b_2 x_2 \\]\n",
    "\n",
    "### Variables\n",
    "- \\( y \\): Weight (dependent variable)\n",
    "- \\( x_1 \\): Height (independent variable)\n",
    "- \\( x_2 \\): Age (independent variable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854c2b2",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073559f3",
   "metadata": {},
   "source": [
    "### Assumptions of Linear Regression\n",
    "\n",
    "1. **Linearity**: The relationship between the independent variables and the dependent variable is linear.\n",
    "2. **Independence**: Observations are independent of each other.\n",
    "3. **Homoscedasticity**: The residuals (errors) have constant variance at every level of the independent variables.\n",
    "4. **Normality of Residuals**: The residuals of the model are normally distributed.\n",
    "5. **No Multicollinearity**: Independent variables are not highly correlated with each other.\n",
    "\n",
    "### Checking Assumptions in a Given Dataset\n",
    "\n",
    "1. **Linearity**:\n",
    "   - **Method**: Plot the observed vs. predicted values or residuals vs. predicted values.\n",
    "   - **Interpretation**: Look for a linear pattern.\n",
    "\n",
    "2. **Independence**:\n",
    "   - **Method**: Check the data collection process to ensure observations are independent.\n",
    "   - **Interpretation**: No specific pattern should be observed in the residual plot.\n",
    "\n",
    "3. **Homoscedasticity**:\n",
    "   - **Method**: Plot residuals vs. predicted values.\n",
    "   - **Interpretation**: Residuals should be randomly scattered without a clear pattern.\n",
    "\n",
    "4. **Normality of Residuals**:\n",
    "   - **Method**: Use a Q-Q plot or histogram of residuals.\n",
    "   - **Interpretation**: Residuals should follow a straight line in the Q-Q plot or form a bell-shaped histogram.\n",
    "\n",
    "5. **No Multicollinearity**:\n",
    "   - **Method**: Calculate Variance Inflation Factor (VIF) for each independent variable.\n",
    "   - **Interpretation**: VIF values should be less than 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0579f",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc82da",
   "metadata": {},
   "source": [
    "### Interpreting Slope and Intercept in a Linear Regression Model\n",
    "\n",
    "1. **Intercept (\\( b_0 \\))**:\n",
    "   - The intercept is the expected value of the dependent variable (\\( y \\)) when all independent variables (\\( x \\)) are zero.\n",
    "   - It represents the starting point or baseline value of \\( y \\).\n",
    "\n",
    "2. **Slope (\\( b_1, b_2, \\ldots \\))**:\n",
    "   - The slope represents the change in the dependent variable (\\( y \\)) for a one-unit change in the independent variable (\\( x \\)), holding other variables constant.\n",
    "   - Each slope corresponds to a specific independent variable.\n",
    "\n",
    "### Example: Real-World Scenario\n",
    "\n",
    "**Scenario**: Predicting house prices based on square footage and number of bedrooms.\n",
    "\n",
    "**Model**: \\( \\text{Price} = b_0 + b_1 \\cdot \\text{SquareFootage} + b_2 \\cdot \\text{Bedrooms} \\)\n",
    "\n",
    "**Variables**:\n",
    "- \\( \\text{Price} \\): Dependent variable (house price)\n",
    "- \\( \\text{SquareFootage} \\): Independent variable (size of the house in square feet)\n",
    "- \\( \\text{Bedrooms} \\): Independent variable (number of bedrooms)\n",
    "\n",
    "**Interpretation**:\n",
    "- **Intercept (\\( b_0 \\))**: The expected price of a house when the square footage and number of bedrooms are both zero. This value might not always make practical sense but is necessary for the model.\n",
    "- **Slope (\\( b_1 \\))**: The change in house price for each additional square foot of the house, assuming the number of bedrooms remains constant.\n",
    "- **Slope (\\( b_2 \\))**: The change in house price for each additional bedroom, assuming the square footage remains constant.\n",
    "\n",
    "### Example Values and Interpretation:\n",
    "Suppose the model outputs are:\n",
    "- \\( b_0 = 50,000 \\) (intercept)\n",
    "- \\( b_1 = 200 \\) (slope for square footage)\n",
    "- \\( b_2 = 15,000 \\) (slope for bedrooms)\n",
    "\n",
    "- **Intercept**: A house with 0 square footage and 0 bedrooms is expected to cost $50,000.\n",
    "- **Slope for Square Footage**: For each additional square foot, the house price increases by $200, holding the number of bedrooms constant.\n",
    "- **Slope for Bedrooms**: For each additional bedroom, the house price increases by $15,000, holding the square footage constant.\n",
    "\n",
    "### Summary\n",
    "In linear regression:\n",
    "- The intercept provides the baseline value of the dependent variable.\n",
    "- The slope indicates the effect of each independent variable on the dependent variable, allowing us to understand and predict changes in the dependent variable based on changes in the independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b258f",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5eb7ea",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "**Concept**:\n",
    "- Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models.\n",
    "- It iteratively adjusts the model parameters (weights) in the direction of the steepest decrease in the cost function.\n",
    "\n",
    "**Steps**:\n",
    "1. **Initialize Parameters**: Start with initial guesses for the model parameters.\n",
    "2. **Compute Gradient**: Calculate the gradient (partial derivatives) of the cost function with respect to each parameter.\n",
    "3. **Update Parameters**: Adjust the parameters in the opposite direction of the gradient by a learning rate.\n",
    "4. **Iterate**: Repeat the process until the cost function converges to a minimum or reaches an acceptable threshold.\n",
    "\n",
    "**Formula**:\n",
    "- Parameter update rule: \\( \\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta) \\)\n",
    "  - \\( \\theta \\): Model parameters\n",
    "  - \\( \\alpha \\): Learning rate\n",
    "  - \\( \\nabla J(\\theta) \\): Gradient of the cost function\n",
    "\n",
    "**Usage in Machine Learning**:\n",
    "- **Training Models**: Used to train models by finding the optimal parameters that minimize the cost function.\n",
    "- **Applications**: Commonly used in linear regression, logistic regression, neural networks, and other machine learning algorithms.\n",
    "\n",
    "**Example**:\n",
    "- In linear regression, gradient descent is used to find the line of best fit by minimizing the sum of squared errors between the predicted and actual values.\n",
    "\n",
    "### Summary\n",
    "Gradient descent is a fundamental optimization technique in machine learning for iteratively updating model parameters to minimize the cost function and improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f7f75",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044c138",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression Model\n",
    "\n",
    "**Definition**:\n",
    "- Multiple linear regression is a statistical technique that models the relationship between one dependent variable and two or more independent variables by fitting a linear equation to observed data.\n",
    "\n",
    "**Model Equation**:\n",
    "\\[ y = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n \\]\n",
    "- \\( y \\): Dependent variable\n",
    "- \\( b_0 \\): Intercept\n",
    "- \\( b_1, b_2, \\ldots, b_n \\): Coefficients (slopes) for the independent variables\n",
    "- \\( x_1, x_2, \\ldots, x_n \\): Independent variables\n",
    "\n",
    "### Differences from Simple Linear Regression\n",
    "\n",
    "1. **Number of Independent Variables**:\n",
    "   - **Simple Linear Regression**: Models the relationship between one dependent variable and one independent variable.\n",
    "   - **Multiple Linear Regression**: Models the relationship between one dependent variable and two or more independent variables.\n",
    "\n",
    "2. **Equation Complexity**:\n",
    "   - **Simple Linear Regression**: \\( y = b_0 + b_1 x \\)\n",
    "   - **Multiple Linear Regression**: \\( y = b_0 + b_1 x_1 + b_2 x_2 + \\cdots + b_n x_n \\)\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - **Simple Linear Regression**: The slope represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "   - **Multiple Linear Regression**: Each slope represents the change in the dependent variable for a one-unit change in the corresponding independent variable, holding other variables constant.\n",
    "\n",
    "4. **Applications**:\n",
    "   - **Simple Linear Regression**: Suitable for modeling simple relationships with one predictor.\n",
    "   - **Multiple Linear Regression**: Suitable for modeling more complex relationships with multiple predictors.\n",
    "\n",
    "### Example\n",
    "**Simple Linear Regression**: Predicting weight based on height.\n",
    "\\[ \\text{Weight} = b_0 + b_1 \\cdot \\text{Height} \\]\n",
    "\n",
    "**Multiple Linear Regression**: Predicting weight based on height and age.\n",
    "\\[ \\text{Weight} = b_0 + b_1 \\cdot \\text{Height} + b_2 \\cdot \\text{Age} \\]\n",
    "\n",
    "### Summary\n",
    "Multiple linear regression extends simple linear regression by including multiple independent variables, allowing for a more comprehensive analysis of the factors affecting the dependent variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e958719",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0280bca3",
   "metadata": {},
   "source": [
    "### Multicollinearity in Multiple Linear Regression\n",
    "\n",
    "**Concept**:\n",
    "- Multicollinearity occurs when two or more independent variables in a multiple linear regression model are highly correlated.\n",
    "- This correlation implies that the independent variables provide redundant information about the dependent variable.\n",
    "\n",
    "**Problems Caused by Multicollinearity**:\n",
    "- **Unstable Coefficients**: Estimates of the regression coefficients become unstable and sensitive to small changes in the model.\n",
    "- **Inflated Standard Errors**: Standard errors of the coefficients are increased, leading to less reliable statistical tests and wider confidence intervals.\n",
    "- **Difficulty in Interpretation**: It becomes challenging to determine the individual effect of each independent variable on the dependent variable.\n",
    "\n",
    "### Detecting Multicollinearity\n",
    "\n",
    "1. **Variance Inflation Factor (VIF)**:\n",
    "   - Calculate the VIF for each independent variable. VIF values greater than 10 indicate high multicollinearity.\n",
    "   - Formula: \\( \\text{VIF} = \\frac{1}{1 - R^2} \\), where \\( R^2 \\) is the coefficient of determination of the regression of one independent variable on all other independent variables.\n",
    "\n",
    "2. **Correlation Matrix**:\n",
    "   - Examine the correlation matrix of the independent variables. High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "\n",
    "3. **Condition Index**:\n",
    "   - Calculate the condition index, which measures the sensitivity of the regression coefficients to small changes in the data. A condition index above 30 suggests severe multicollinearity.\n",
    "\n",
    "### Addressing Multicollinearity\n",
    "\n",
    "1. **Remove Highly Correlated Variables**:\n",
    "   - Identify and remove one of the highly correlated variables to reduce redundancy.\n",
    "\n",
    "2. **Combine Variables**:\n",
    "   - Combine highly correlated variables into a single composite variable (e.g., using principal component analysis).\n",
    "\n",
    "3. **Ridge Regression**:\n",
    "   - Use ridge regression, a technique that adds a penalty to the regression model to shrink the coefficients and reduce multicollinearity.\n",
    "\n",
    "4. **Centering the Variables**:\n",
    "   - Center the variables by subtracting the mean of each variable from the data points to reduce multicollinearity.\n",
    "\n",
    "### Example Code to Detect Multicollinearity in Python\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame with independent variables\n",
    "X = df[['x1', 'x2', 'x3']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Calculate VIF for each independent variable\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "\n",
    "print(vif_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75835816",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e4bcc1",
   "metadata": {},
   "source": [
    "### Polynomial Regression Model\n",
    "\n",
    "**Definition**:\n",
    "- Polynomial regression is a form of linear regression in which the relationship between the independent variable \\( x \\) and the dependent variable \\( y \\) is modeled as an \\( n \\)-th degree polynomial.\n",
    "- Unlike simple linear regression, which fits a straight line, polynomial regression fits a curve to the data.\n",
    "\n",
    "**Model Equation**:\n",
    "\\[ y = b_0 + b_1 x + b_2 x^2 + \\cdots + b_n x^n \\]\n",
    "- \\( y \\): Dependent variable\n",
    "- \\( b_0, b_1, b_2, \\ldots, b_n \\): Coefficients\n",
    "- \\( x \\): Independent variable\n",
    "\n",
    "### Differences from Linear Regression\n",
    "\n",
    "1. **Nature of Relationship**:\n",
    "   - **Linear Regression**: Assumes a linear relationship between the independent and dependent variables.\n",
    "   - **Polynomial Regression**: Models a nonlinear relationship by fitting a polynomial equation.\n",
    "\n",
    "2. **Equation**:\n",
    "   - **Linear Regression**: \\( y = b_0 + b_1 x \\)\n",
    "   - **Polynomial Regression**: \\( y = b_0 + b_1 x + b_2 x^2 + \\cdots + b_n x^n \\)\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - **Linear Regression**: Limited to straight-line relationships.\n",
    "   - **Polynomial Regression**: Can model more complex, curved relationships, providing greater flexibility in fitting the data.\n",
    "\n",
    "4. **Overfitting Risk**:\n",
    "   - **Linear Regression**: Less prone to overfitting due to its simplicity.\n",
    "   - **Polynomial Regression**: Higher risk of overfitting, especially with high-degree polynomials, as the model may fit the noise in the data.\n",
    "\n",
    "### Example: Real-World Scenario\n",
    "\n",
    "**Linear Regression Example**:\n",
    "- Predicting a person's salary based on their years of experience.\n",
    "\\[ \\text{Salary} = b_0 + b_1 \\cdot \\text{YearsOfExperience} \\]\n",
    "\n",
    "**Polynomial Regression Example**:\n",
    "- Predicting a car's fuel efficiency based on its speed.\n",
    "\\[ \\text{FuelEfficiency} = b_0 + b_1 \\cdot \\text{Speed} + b_2 \\cdot \\text{Speed}^2 \\]\n",
    "- This model captures the nonlinear relationship where fuel efficiency may initially increase with speed but then decrease at higher speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cec210",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f58b8",
   "metadata": {},
   "source": [
    "### Advantages and Disadvantages of Polynomial Regression Compared to Linear Regression\n",
    "\n",
    "**Advantages of Polynomial Regression**:\n",
    "1. **Flexibility**: Can model nonlinear relationships between the independent and dependent variables.\n",
    "2. **Better Fit**: Can provide a better fit for data with curvature, capturing more complex patterns.\n",
    "\n",
    "**Disadvantages of Polynomial Regression**:\n",
    "1. **Overfitting**: Higher risk of overfitting, especially with high-degree polynomials, which can fit the noise in the data.\n",
    "2. **Complexity**: More complex than linear regression, making it harder to interpret the model.\n",
    "3. **Computational Cost**: Higher computational cost due to the increased number of parameters.\n",
    "4. **Extrapolation**: Poor performance in extrapolation beyond the range of the data, as the polynomial can produce unrealistic predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee69f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
